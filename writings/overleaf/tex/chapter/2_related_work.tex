\chapter{Related work}
\label{chap:related_work}

% \section{Basic models and knowledge}

Laying out why the following sections are relevant for this work


% \subsection{Camera model}
% \begin{enumerate}
%     \item Homogenous coordinates
%     \item Pinhole camera model
%     \item Intrinsics
%     \item Extrinsics
%     \item Distortion
% \end{enumerate}


\section{Scene representation}

The problem of scene reconstruction is an extensive challenge consisting of different aspects,
one of which is scene representation.
It defines the way how scene features are described.
Different representations have its own properties and can be a better or worse fit depending on the task.

Scene features usually include geometry and illumination information
and can be represented \textit{explicitly} or \textit{implicitly}.

\subsection{Explicit geometry}

\textit{Explicit} methods use geometric primitives to describe scenes.

\textit{Voxel grids} (\cite{Lombardi_2019}) is a very common way to describe a geometry of the scene mostly due to its simplicity.
However, straightforwardly they are highly memory demanding.
This can be handled applying techniques like multi-resolution approach (\cite{hÃ¤ne2017hierarchical}), octree hierarchies (\cite{riegler2017octnet}, \cite{tatarchenko2017octree})
or truncated signed distance fields (\cite{truncdistfield1996curless}),
which in fact already represent geometry implicitly.

Another way to represent a geometry is using \textit{point clouds} (\cite{qi2017pointnet}, \cite{fan2016point}).
They can be easily retrieved using different sensors (e.g. depth cameras),
therefore they are widely used in robotics and computer graphics fields.
Point clouds require a complex post-processing step
(e.g. \cite{ballpivoting1999bernardini}) in order to produce the mesh of the scene,
which makes them quite tedious to work with.

Geometry can be represented using \textit{meshes},
where corresponding edges and vertices form a graph (\cite{wang20183d}).
Although these methods work directly on meshes,
the downsides are in high limitations of these methods,
such as requirement of reference template mesh,
tendency to produce self-intersecting meshes or opened surfaces (\cite{groueix2018atlasnet}).

\subsection{Implicit geometry}

\textit{Implicit} methods map points in space to some value,
which implicitly gives knowledge about the scene.

The most common example of implicit geometry representation is the signed distance field (SDF) (\cite{truncdistfield1996curless}, \cite{Lombardi_2019}),
which is basically a mapping $\mathbb{R}^3 \xrightarrow{} \mathbb{R}$ defining the surface as a level-set (mostly a zero-based).

Occupancy fields (\cite{occupancy2019mescheder}) are introduced as neural networks
that directly learn continuous 3D occupancy function.
This allows to achieve an arbitrary resolution
instead of learning voxelized representations
that results only in fixed resolution.


\im{Eigher reference brdf models here, or as it comes to them}




\section{Neural scene representation}

Neural scene representations exploit the idea of employing
deep neural networks in order to implicitly encode information about the scene.
The network learns mapping between the spatial locations and feature representations of the scene.
The rendering techniques is used in order to get novel views of the scene.
To make training possible the rendering process has to be formulated in a differentiable way.
This allows to use loss functions that minimizes the differences between generated views and ground truth 2D images.

\cite{niemeyer2020differentiable} introduces the volumetric rendering method,
that uses the differentiable rendering functions,
allowing the implicit neural geometry representations to be optimized
with only 2D supervision instead of requiring 3D ground truth models.
Using only 2D sample images the predicted surface depth is used
for projecting and sampling the ray.
Resulting intersection locations are then forwarded into texture prediction field,
which outputs color value for that point.
The appearance is not explicitly modelled, meaning that the view- and lighting-dependant effect are not taken into account.


The Scene Representations Networks (SRNs) are introduced by \cite{sitzmann2019srns}.
The recurrent neural networks lie in the core of this approach.
Camera extrinsics and intrinsics is passed as inputs into the network,
which is then being evaluated for all the samples along the ray,
trying to decided where the surface is located.
The network produces the feature vector which is then decoded into a single color value for that point on the surface.
This method implies no explicit appearance modelling.
The SRNs approach grows from DeepVoxel method (\cite{sitzmann2019deepvoxels}),
where the feature vectors are integrated into a persistent cartesian 3D-voxel grid.
% The similar approach is used in \cite{saito2019pifu},
% where 2 separate predictors are used for occupancy and texture fields.

Local Light Field Fusion (LLFF) (\cite{mildenhall2019local}) ... --> NeRF



\sm{sounds like a good listing}
{\color{teal}
\begin{enumerate}
    \item Explicit/implicit scene representations
    \item Neural Radiance Field (Scene Representation Networks, Local Light Field Fusion ..->.. NeRF)
    \item Positional encoding (Fourier Features Let Networks Learn...) % How it is performed and what for? The background research behind this.
    \item NeRF optimizations (NSVF, FastNeRF, KiloNeRFAutomatic Integration and others)
    \item Rendering under novel lighting conditions (NRF, DRF, Deep Voxels, NeRD, NeRV, NeRFactor etc)
\end{enumerate}
}


% \lipsum[1-15]