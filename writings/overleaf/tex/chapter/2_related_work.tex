\chapter{Related work}
\label{chap:related_work}

% \section{Basic models and knowledge}

This chapter reviews previous related works as well as concurrent to this thesis works.


% \subsection{Camera model}
% \begin{enumerate}
%     \item Homogenous coordinates
%     \item Pinhole camera model
%     \item Intrinsics
%     \item Extrinsics
%     \item Distortion
% \end{enumerate}


\section{Scene representation}

The problem of scene reconstruction is an extensive challenge consisting of different aspects,
one of which is scene representation.
It defines the way how scene features are described.
Different representations have their own properties and can be a better or worse fit depending on the task.

The scene features usually include geometry and illumination information
and can be represented \textit{explicitly} or \textit{implicitly}.

\subsection{Explicit geometry}

\textit{Explicit} methods use geometric primitives to describe scenes.

\textit{Voxel grids} (\cite{Lombardi_2019}) is a very common way to describe a geometry of the scene mostly due to its simplicity.
However, straightforwardly they are highly memory demanding.
This can be handled applying techniques like multi-resolution approach (\cite{hÃ¤ne2017hierarchical}), octree hierarchies (\cite{riegler2017octnet, tatarchenko2017octree})
or truncated signed distance fields (\cite{truncdistfield1996curless}),
which in fact already represent geometry implicitly.

Another way to represent a geometry is using \textit{point clouds} (\cite{qi2017pointnet, fan2016point}).
They can be easily retrieved using different sensors (e.g. depth cameras),
therefore they are widely used in robotics and computer graphics fields.
Point clouds require a complex post-processing step
(e.g. \cite{ballpivoting1999bernardini}) in order to produce the mesh of the scene,
which makes them quite tedious to work with.

% \im{(sm): \textit{why do we need/want a mesh?}}
Geometry can be represented using \textit{meshes},
where corresponding edges and vertices form a graph (\cite{wang20183d}).
Although, meshes have advantages (e.g. they are good for forward rendering),
the downsides of methods that are using meshes appear in high limitations
(e.g. AtlasNet approach \cite{groueix2018atlasnet} tends to produce self-intersecting meshes or opened surfaces)
% Although these methods \im{(sm): \textit{which methods?}} work directly on meshes,
% the downsides are in high limitations of these methods,
% such as requirement of \im{reference template mesh (sm): ??},
% tendency to produce self-intersecting meshes or opened surfaces (\cite{groueix2018atlasnet}).

\subsection{Implicit geometry}

\textit{Implicit} methods map points in space to some value,
which implicitly gives knowledge about the scene.

The most common example of implicit geometry representation is the signed distance field (SDF) (\cite{truncdistfield1996curless, Lombardi_2019}),
which is basically a mapping $\mathbb{R}^3 \xrightarrow{} \mathbb{R}$ defining the surface as a level-set (mostly a zero-based).

Occupancy fields (\cite{occupancy2019mescheder}) are introduced as neural networks
that directly learn continuous 3D occupancy function.
This allows to achieve an arbitrary resolution and more compact and resolution-independent representation
instead of learning voxelized representations
that results only in fixed resolution.


% \im{Either reference brdf models here, or as it comes to them}




\section{Neural scene representation}

Neural scene representations exploit the idea of employing
deep neural networks in order to implicitly encode information about the scene.
The network learns mapping between the spatial locations and feature representations of the scene.
% The rendering techniques is used in order to get novel views of the scene.
Different rendering techniques can be applied to get novel views of the scene
but the volumetric rendering technique is the most commonly used with neural scene representations.
To make training possible the rendering process has to be formulated in a differentiable way.
This allows to use loss functions that minimizes the differences between generated views and ground truth 2D images.

\cite{niemeyer2020differentiable} introduce a volumetric rendering method,
that uses differentiable rendering functions,
allowing the implicit neural geometry representations to be optimized
with only 2D supervision instead of requiring 3D ground truth models.
Using only 2D sample images the surface depth map is predicted
and is then unprojected into 3D to be evaluated in texture prediction field,
which outputs color values.
% Resulting intersection locations are then forwarded into texture prediction field,
% which outputs color value for that point.
The appearance is not explicitly modelled, meaning that the view- and lighting-dependant effect are not taken into account.


The Scene Representations Networks (SRNs) are introduced by \cite{sitzmann2019srns}.
Recurrent neural networks lie in the core of this approach.
Camera extrinsics and intrinsics are passed as inputs into the network,
which is then evaluated for all the samples along the ray,
trying to decided where the surface is located.
The network produces the feature vector which is then decoded into a single color value for that point on the surface.
This method implies no explicit appearance modelling.
The SRNs approach grows from DeepVoxel method (\cite{sitzmann2019deepvoxels}),
where the feature vectors are integrated into a persistent cartesian 3D-voxel grid.
% The similar approach is used in \cite{saito2019pifu},
% where 2 separate predictors are used for occupancy and texture fields.

\cite{mildenhall2019local} propose to expand each sample view into local light field (LLF) using multiplane images (MPIs).
The novel view is then rendered by projecting four nearest neighbors of the adjacent local light fields.
This work is followed by Neural Radiance Fields (NeRF) \cite{mildenhall2020nerf} proposed by same already introduced in this text earlier authors.
NeRF extends the LLFs in a different way by leveraging volume rendering.
The fully differentiable pipeline allows to optimize parameters of deep neural networks,
which predict the volume density and the scene color at given 3D point.
The volume rendering technique is then used for gathering these sample points along the ray
and calculating color values for the novel view.
NeRF lies in the the basis of the proposed in this work methods.

The downside of the NeRFs is the inefficiency of this method.
\cite{liu2021neural} proposes to use the octree as an underlying structure for the scene,
which allows to increase the efficiency of sampling at the cost of very efficient ray voxel intersections.
Authors also propose to store learnable feature vectors in voxel corners,
which allows to decrease the size of network while keeping its capacity.
The concept of using sparse voxel octrees is used in all of the proposed in this thesis methods.

In FastNeRF \cite{garbin2021fastnerf} propose factorization of deep radiance maps
that can be cached and queried on high-end GPUs.
This technique extends the original NeRF network structre
by two separate MLPs for position-dependent and direction-dependent inputs.
The evaluation time can be decreased by up to 4000 times.
The method is not able to relight the scene.

\cite{reiser2021kilonerf} with the KiloNeRF approach employ thousand of tiny MLPs 
instead of one large MLP as proposed in NeRF.
Each of these MLPs is responsible for its own part of the scene
and all of them can be processed simultaniously on modern GPUs.
This work reaches more than 2000 times faster in both training and inference stages,
does not consider any light interaction though.

\cite{yu2021plenoctrees} propose PlenOctrees as another structure that can be used to increase the efficiency of original NeRF method.
Authors separate the original network to factorize the appearance using closed-form
spherical basis functions - spherical harmonics (SH) \cite{mohlenkamp1997spherical}.
The SH coefficients together with volume density values are then used
for building the PlenOctree.
Although faster inference time is achieved, novel light conditions are not considered.

In DeRF approach \cite{rebain2020derf} the scene is separated into parts by applying spatial decomposition.
These parts are then processed with smaller networks, which allows to achieve near-constant inference time.
The outputs of these networks are later composited using Painter's algorithm \cite{deBerg2008}.

Another way to speed NeRFs up is to rethink the integral estimation using Monte Carlo sampling.
AutoInt \cite{lindell2021autoint} estimate the rendering integral from NeRF
by building grad networks that can estimate any definite integral in two evaluations of the network.
With this approach the training time can be 10 times smaller comparing to original NeRF.

\cite{zhang2020nerf} elaborate on shape-radiance ambiguity
and propose the NeRF++ approach, which uses a separate NeRF to predict background,
which increases performance for the unbounded scenes where the dynamic depth range can be very high.

\cite{lin2021barf} propose BARF approach that is aimed to solve joint problem
of learning neural 3D representations together with reconstructing unknown camera poses.
This technique removes the limitation for the necessity of known camera views.

\subsection{Relighting}

Another branch of works focuses on extending the relighting limitations for NeRF-based approaches.
\cite{martinbrualla2021nerfw} was one of the first works based on original NeRF approach.
The proposed technique with more complex model structure
that separately renders static and transient elements of the scene
and then being composited.
The key idea is to optimize uncertainty image, which is used to weigh target image.
The difference between this target image and the composited outputs is minimized during training.
The proposed solution is able to outperform original NeRF in terms of quality
when applied for the datasets with less controlled multi-view collections.
This approach allows to interpolate color and illumination between two training images,
which add relighting capabilities, which are highly limited though.

\cite{bi2020neural} reformulates the rendering equation for considering a single non-static light source on the scene.
Although, in general this is highly impractical, one specific case
when the light source is co-located with the camera can be evaluated
without any sufficient overhead comparing to the NeRF work.
This approach implies the appearance to be modelled using any differentiable BRDF model.
In former work \cite{bi2020deep} the authors reconstructs discrete reflectance volumes,
which results in its main limitation of fixed volume resolution.
The formulation of rendering equation from NRF
lies in the basis of all of proposed in this thesis methods except the implicit scheme \textit{ImNRF}.

\cite{nerv2021} with the NeRV approach addresses the relighting problem
by extending the original NeRF method with a second MLP
that predicts a visibility field.
The 'one-bounce' indirect illumination is considered in this approach.
This is a concurrent to this thesis work, which results are presented in \Cref{fig:concur_nerv} for performance comparison.

Neural Reflectance Decomposition (NeRD) \cite{boss2020nerd} uses two networks:
'coarse' Sampling and 'fine' Decomposition networks to decompose the scene into
spatially varying BRDF material properties.
This concurrent approach consider uncontrolled illumination 
and is able to relight the scene under novel lighting conditions.
The results of this work are shown in \Cref{fig:concur_nerd} for comparison with results of this work.

In NeRFactor \cite{nerfactor} the surface representation is extracted from the NeRF scene representation.
Scene geometry is refined simultaneously with the spatially-varying reflectance and environment lighting.
This approach does not have requirement for known illumination
and allows for novel light-view synthesis.
Some selective results of this concurrent approach are shown in \Cref{fig:concur_nerfactor1} and \Cref{fig:concur_nerfactor2}



% \cite{Zhang2021lighttransport} introduce a semi-parametric approach
% for learning light transports using neural representations.
% Authors use special hardware setup for capturing training images under known illumination.
% The approach consists of observation- and query-paths.
% The observation path encodes K nearby observations into multiscale features,
% which are then concatenated and used in the query path as feature activations.
% The query path predicts residual maps from novel light and view directions
% and resample the texture-space rendering back to the camera space
% where it is supervised by the ground truth images.

% \im{ref to sebastian's review}



% \sm{sounds like a good listing}
% {\color{teal}
% \begin{enumerate}
%     \item Explicit/implicit scene representations
%     \item Neural Radiance Field (Scene Representation Networks, Local Light Field Fusion ..->.. NeRF)
%     \item Positional encoding (Fourier Features Let Networks Learn...) % How it is performed and what for? The background research behind this.
%     \item NeRF optimizations (NSVF, FastNeRF, KiloNeRFAutomatic Integration and others)
%     \item Rendering under novel lighting conditions (NRF, DRF, Deep Voxels, NeRD, NeRV, NeRFactor etc)
% \end{enumerate}
% }


% \lipsum[1-15]