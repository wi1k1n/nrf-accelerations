\chapter{Related work}
\label{chap:related_work}

% \section{Basic models and knowledge}

Laying out why the following sections are relevant for this work


% \subsection{Camera model}
% \begin{enumerate}
%     \item Homogenous coordinates
%     \item Pinhole camera model
%     \item Intrinsics
%     \item Extrinsics
%     \item Distortion
% \end{enumerate}


\section{Scene representation}

The problem of scene reconstruction is an extensive challenge consisting of different aspects,
one of which is scene representation.
It defines the way how scene features are described.
Different representations have its own properties and can be a better or worse fit depending on the task.

Scene features usually include geometry and illumination information
and can be represented \textit{explicitly} or \textit{implicitly}.

\subsection{Explicit geometry}

\textit{Explicit} methods use geometric primitives to describe scenes.

\textit{Voxel grids} (\cite{Lombardi_2019}) is a very common way to describe a geometry of the scene mostly due to its simplicity.
However, straightforwardly they are highly memory demanding.
This can be handled applying techniques like multi-resolution approach (\cite{h√§ne2017hierarchical}), octree hierarchies (\cite{riegler2017octnet, tatarchenko2017octree})
or truncated signed distance fields (\cite{truncdistfield1996curless}),
which in fact already represent geometry implicitly.

Another way to represent a geometry is using \textit{point clouds} (\cite{qi2017pointnet, fan2016point}).
They can be easily retrieved using different sensors (e.g. depth cameras),
therefore they are widely used in robotics and computer graphics fields.
Point clouds require a complex post-processing step
(e.g. \cite{ballpivoting1999bernardini}) in order to produce the mesh of the scene,
which makes them quite tedious to work with.

Geometry can be represented using \textit{meshes},
where corresponding edges and vertices form a graph (\cite{wang20183d}).
Although these methods work directly on meshes,
the downsides are in high limitations of these methods,
such as requirement of reference template mesh,
tendency to produce self-intersecting meshes or opened surfaces (\cite{groueix2018atlasnet}).

\subsection{Implicit geometry}

\textit{Implicit} methods map points in space to some value,
which implicitly gives knowledge about the scene.

The most common example of implicit geometry representation is the signed distance field (SDF) (\cite{truncdistfield1996curless, Lombardi_2019}),
which is basically a mapping $\mathbb{R}^3 \xrightarrow{} \mathbb{R}$ defining the surface as a level-set (mostly a zero-based).

Occupancy fields (\cite{occupancy2019mescheder}) are introduced as neural networks
that directly learn continuous 3D occupancy function.
This allows to achieve an arbitrary resolution
instead of learning voxelized representations
that results only in fixed resolution.


\im{Either reference brdf models here, or as it comes to them}




\section{Neural scene representation}

Neural scene representations exploit the idea of employing
deep neural networks in order to implicitly encode information about the scene.
The network learns mapping between the spatial locations and feature representations of the scene.
The rendering techniques is used in order to get novel views of the scene.
To make training possible the rendering process has to be formulated in a differentiable way.
This allows to use loss functions that minimizes the differences between generated views and ground truth 2D images.

\cite{niemeyer2020differentiable} introduce the volumetric rendering method,
that uses the differentiable rendering functions,
allowing the implicit neural geometry representations to be optimized
with only 2D supervision instead of requiring 3D ground truth models.
Using only 2D sample images the predicted surface depth is used
for projecting and sampling the ray.
Resulting intersection locations are then forwarded into texture prediction field,
which outputs color value for that point.
The appearance is not explicitly modelled, meaning that the view- and lighting-dependant effect are not taken into account.


The Scene Representations Networks (SRNs) are introduced by \cite{sitzmann2019srns}.
The recurrent neural networks lie in the core of this approach.
Camera extrinsics and intrinsics are passed as inputs into the network,
which is then evaluated for all the samples along the ray,
trying to decided where the surface is located.
The network produces the feature vector which is then decoded into a single color value for that point on the surface.
This method implies no explicit appearance modelling.
The SRNs approach grows from DeepVoxel method (\cite{sitzmann2019deepvoxels}),
where the feature vectors are integrated into a persistent cartesian 3D-voxel grid.
% The similar approach is used in \cite{saito2019pifu},
% where 2 separate predictors are used for occupancy and texture fields.

\cite{mildenhall2019local} propose to expand each sample view into local light field (LLF) using the multiplane images (MPIs).
The novel view is then rendered by projecting four nearest neighbors of the adjacent local light fields.
This work is followed by Neural Radiance Fields (NeRF) \cite{mildenhall2020nerf} proposed by same authors.
NeRF extends the LLFs in a different way by leveraging the volume rendering.
The fully differentiable pipeline allows to optimize parameters of deep neural networks,
which predict the volume density and the scene color at given 3D point.
The volume rendering technique is then used for gathering these sample points along the ray
and calculating color values for the novel view.

The downside of the NeRFs is the inefficiency of this method.
\cite{liu2021neural} proposes to use the octree as an underlying structure for the scene,
which allows to increase the efficiency of sampling at the cost of very efficient ray voxel intersections.
Authors also propose to store learnable feature vectors in voxel corners,
which allows to decrease the size of network while keeping its capacity.

In FastNeRF \cite{garbin2021fastnerf} propose factorization of deep radiance maps
that can be cached and queried on the high-end GPUs.
This technique extends the original NeRF network structre
by two separate MLPs for position-dependent and direction-dependent inputs.
The evaluation time can be decreased by up to 4000 times.

\cite{reiser2021kilonerf} with the KiloNeRF approach employ thousand of tiny MLPs 
instead of one large MLP as proposed in NeRF.
Each of these MLPs is responsible for its own part of the scene
and all of them can be processed simultaniously on modern GPUs.
This work reaches more than 2000 times faster in both training and inference stages.

\cite{yu2021plenoctrees} propose PlenOctrees as another structure that can be used to increase the efficiency of original NeRF method.
Authors separate the original network to factorize the appearance using closed-form
spherical basis functions - spherical harmonics (SH) \cite{mohlenkamp1997spherical}.
The SH coefficients together with volume density values are then used
for building the PlenOctree.

In DeRF approach \cite{rebain2020derf} the scene is separated into parts by applying spatial decomposition.
These parts are then processed with smaller networks, which allows to achieve near-constant inference time.
The outputs of these networks are later composited using Painter's algorithm \cite{deBerg2008}.

Another way to speed NeRFs up is to rethink the integral estimation using Monte Carlo sampling.
AutoInt \cite{lindell2021autoint} estimate the rendering integral from NeRF
by building grad networks that can estimate any definite integral in two evaluations of the network.
With this approach the training time can be 10 times smaller comparing to original NeRF.

\cite{zhang2020nerf} elaborate on shape-radiance ambiguity
and propose the NeRF++ approach, which uses a separate NeRF to predict background,
which increases performance for the unbounded scenes where the dynamic depth range can be very high.




Another branch of works focuses on extending the relighting limitations for NeRF-based approaches.
\cite{martinbrualla2021nerfw} was one of the first works based on original NeRF approach.
The proposed technique with more complex model structure
that separately renders static and transient elements of the scene
and then being composited.
The key idea is to optimize uncertainty image, which is used to weigh target image.
The difference between this target image and the composited outputs is minimized during training.
The proposed solution is able to outperform original NeRF in terms of quality
when applied for the datasets with less controlled multi-view collections.

\cite{bi2020neural} reformulates the rendering equation for considering a single light source on the scene.
Although, in general this is highly impractical, one specific case
when the light source is co-located with the camera can be evaluated
without any sufficient overhead comparing to the NeRF work.
This approach implies the appearance to be modelled using any differentiable BRDF model.
In former work \cite{bi2020deep} the authors reconstructs discrete reflectance volumes,
which results in its main limitation of fixed volume resolution.

\cite{nerv2021} with the NeRV approach addresses the relighting problem
by extending the original NeRF method with a second MLP
that predicts the visibility field.
The 'one-bounce' indirect illumination is considered in this approach.

Neural Reflectance Decomposition (NeRD) \cite{boss2020nerd} uses two networks:
'coarse' Sampling and 'fine' Decomposition networks to decompose the scene into
spatially varying BRDF material properties.
The approach does not limit illumination conditions.

In NeRFactor \cite{nerfactor} the surface representation is extracted from the NeRF scene representation.
Scene geometry is refined simultaneously with the spatially-varying reflectance and environment lighting.



\cite{Zhang2021lighttransport} introduce a semi-parametric approach
for learning light transports using neural representations.
Authors use special hardware setup for capturing training images under known illumination.
The approach consists of observation and query paths.
The observation path encodes K nearby observations into multiscale features,
which are then concatenated and used in the query path as feature activations.
The query path predicts residual maps from novel light and view directions
and resample the texture-space rendering back to the camera space
where it is supervised by the ground truth images.





% \sm{sounds like a good listing}
% {\color{teal}
% \begin{enumerate}
%     \item Explicit/implicit scene representations
%     \item Neural Radiance Field (Scene Representation Networks, Local Light Field Fusion ..->.. NeRF)
%     \item Positional encoding (Fourier Features Let Networks Learn...) % How it is performed and what for? The background research behind this.
%     \item NeRF optimizations (NSVF, FastNeRF, KiloNeRFAutomatic Integration and others)
%     \item Rendering under novel lighting conditions (NRF, DRF, Deep Voxels, NeRD, NeRV, NeRFactor etc)
% \end{enumerate}
% }


% \lipsum[1-15]