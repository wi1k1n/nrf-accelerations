
\chapter{Evaluation}
\label{chap:evaluation}

This chapter addresses the implementation details,
datasets, metrics and experiments with the demonstration of the achieved results.



\section{Training and validation data}
\label{sec:datasets}

Each training session consists of optimizing parameters of a separate neural representation from one of the proposed methods.
The dataset for each training session is created on one specific scene and with one specific light setting.
All of the used datasets have been created in an artificial environment using Blender \cite{blender}.
Each dataset consists of rendered RGB images of the scene.
Each image is an HDR image (the OpenEXR \cite{openexr} format is used to store HDR data on disk).
The dataset samples are the aforementioned images
that correspond to a view from a pin-hole camera with intrinsic parameters $\mathcal{I}$ (same for the whole dataset).
The camera is placed on a sphere around the scene,
up-vector is always positive in $Z$ axis,
the orientation is always aimed to the center of the scene.
% The position of the camera is described with two angles: azimuthal angle $\theta$ and inclination angle $\phi$.
The position of the camera is described with two angles: azimuthal angle $\theta$ and inclination angle $\phi$.
For each view, the values of $(\theta, \phi)$ are sampled uniformly on the surface of the sphere.
% In the experiments the inclination angle is bounded between $-10$ and $80$ degrees.
In the experiments, the inclination angle is bounded between $10$ and $100$ degrees.
Camera extrinsics (in form of a transformation matrix) are exported for each dataset sample along with the HDR image.
Additionally to that, the bounding box is also exported as it is required
to initialize the octree for the NSVF method and all of the proposed methods as well.
$200$ sample images are used in datasets with 'static' and 'colocated' light settings.
For the 'arbitrary' light setting each dataset consists of $500$ sample images
for having a more dense light-view sampling distribution.
These sample images are then split into training (75\%) and validation (25\%) sets.


\subsection{Light settings}

Different light settings are required due to a diversity of tasks that different methods are to solve.
The illumination of the scene is set using a point light source.
The intensity of the light source varies for different scenes and is chosen empirically.
The positioning of the light source is performed according to the chosen light setting.
Three different light settings are used in order to create three versions of each dataset (\Cref{fig:light_settings}).
This is done based on the fact that different methods can only handle datasets with different settings:
\begin{enumerate}
    \item \textbf{Static setting}
    
    In this case the light source is positioned at the same global location for all of the dataset samples.
    This allows producing renders of the scene with static illumination.
    The motivation for this type of datasets comes from the limitation of the NSVF method
    as it does not imply dynamic illumination.
    Datasets with static setting are made for 'Vanilla NSVF' (\cite{liu2021neural})
    and can also be processed with 'Brute-force' (\Cref{sec:explicit_scheme})
    and both explicit and implicit schemes (\Cref{fig:implicit_scheme})
    with in-voxel approximation (\Cref{sec:invoxel}).
    
    \item \textbf{Colocated setting}
    
    This setting implies the light source to be co-located with the camera.
    In this case, there is no displacement between the camera and the light source,
    which allows the NRF (\cite{bi2020neural}) approach to reuse the view volume transmittances
    in calculations as transmittances of the light rays.
    The 'Brute-force' scheme as well as both explicit and implicit schemes
    with in-voxel approximation are also able to be trained on this type of datasets.
    
    \item \textbf{Arbitrary setting}
    
    Datasets that have been created with this setting consist of renders
    where both camera and light positions are sampled uniformly on a sphere around the scene.
    The positioning of the light source follows the same scheme as for camera position.
    Another constraint is applied here that forces the angle
    between view and light rays to the center of the scene to be smaller
    than some value $\alpha_{max}$.
    In the experiments, the value $\alpha_{max} = 120$ degrees has been used.
    
    This setting is focused on the general case of the NRF approach
    and can be handled by proposed methods, such as: 'Brute force' scheme
    and both explicit and implicit schemes with in-voxel approximation.
\end{enumerate}

\input{tex/objects/light_settings}





\subsection{Scenes}
\label{subsec:scenes}

Creating the datasets from scratch is motivated by the fact that
the vast majority of related works do not consider any light interaction.
This results in such a problem that the datasets that have been used in these works
do not include any necessary light information (global position).
Another limitation of these datasets is that the light source is static upon the dataset samples,
which results in a sparse light-view sampling of the scene.
To the date of writing this thesis, the datasets from corresponding related works \cite{nerv2021}
still are not publicly available.
Four different scenes have been used for the evaluation (\Cref{fig:dataset_preview}).


% \CatchFileDef{\catchdatasetpreview}{tex/objects/figure_dataset_preview.tex}{}
% \catchdatasetpreview
\input{tex/objects/figure_dataset_preview}

% \textbf{Sphere}

% The \textit{sphere} scene consists solely of a sphere,
% which shares Lambertian reflectance (\cite{blinn1982light}) with some specular and roughness components.
% This scene is mostly used as a baseline as it is a very simple
% both geometrically and in terms of appearance.

\textbf{Rocket}

The \textit{rocket} scene (\Cref{fig:dataset_preview} (a)) contains a single object,
which exhibits geometry of medium complexity and realistic non-Lambertian materials.
This scene contains global geometry complications (rocket wings) and some fine details (rivets on the rocket shell).
The surface of the rocket shows a moderate level of specularities (in comparison with sphere scene)

\textbf{Guitar}

% The \textit{guitar} scene (\Cref{fig:dataset_preview} (b)) is created for testing model's ability
% to reproduce highly specular surface behavior.
% Complex geometry also allows to asses how model handles geometrical features on different scales.
% For example strings can be barely seen on renders and they would presumably the most troublesome geometrical part of the whole scene.

The \textit{guitar} scene (\Cref{fig:dataset_preview} (b)) is created for testing the model's ability
to reproduce complex geometrical features on different scales.
For example, strings can be barely seen on renders and they would presumably be the most troublesome geometrical part of the whole scene.
A fairly specular surface appearance also allows assessing how well does it model surface reflectance behavior.

\textbf{Lego}

\textit{Lego} scene (\Cref{fig:dataset_preview} (c)) is the reproduction of the model
that has been used in a former NeRF work \cite{mildenhall2020nerf}.
The dataset is recreated under a specific illumination,
which consists of only one point light source.
The object exhibits a very complex geometry on all scales.
The appearance of the surface does not show any complications.

\textbf{Hotdog}

\textit{Hotdog} scene (\Cref{fig:dataset_preview} (d)) is also a reproduction of the model
from the NeRF work with only one point light source.
The reflectance behavior of objects on this scene is more sophisticated
comparing to the \textit{lego} scene while the geometry is simpler.


\subsection{Real-world dataset handling}

For the group of methods that only consider static illumination of the scene,
the dataset acquisition can be performed in different ways.
One of these ways is to use a cell phone with its camera to capture the scene.
The acquired images can then be processed with such techniques as
structure-from-motion (\cite{Moulon2012, Jancosek2011, schoenberger2016structure})
in order to retrieve the camera intrinsics and extrinsics in correspondence with captured images.
Another way is to use some special hardware setups with calibrated camera positions
% such as gonioreflectometers or e.g. x-rite tac7 (\cite{merzbach2017highquality}).
such as gonioreflectometers.

However, for the methods that model light interaction the light extrinsics information is lacking
when using the aforementioned techniques.
The acquisition of the dataset consisting of captures from real-world scenes
for this group of methods is a fairly hard task that most probably cannot be performed without any hardware assistance.
Even for the 'colocated' case of the NRF method cellphones could only have been used
together with the robotic arm holding the cellphone (\cite{bi2020neural}).
Some hardware setups such as X-Rite tac7 \cite{merzbach2017highquality} are equipped with static cameras and light sources
and a turntable, which allows to rotate the object and thereby acquire varying viewing and illumination directions.


Another key point is the background that has to be as homogeneous as possible.
The quality of final renders is sensitive to the too complex background behind the object on the scene.



% \begin{enumerate}
%     \item Synthetic datasets:
%     \begin{enumerate}
%         \item static/co-located/arbitrary
%         \item Blender + python
%         \item Blender point light intensity for light attenuation
%         \item Different scenes (with its features explanations: reflectance, shape, etc.)
%     \end{enumerate}
%     \item Real-world datasets: flower??? tac7
% \end{enumerate}


\section{Implementation details}

% All of the proposed methods are implemented on Python
% using PyTorch \cite{pytorch} and Fairseq \cite{ott2019fairseq} frameworks
% for neural fields and ray casting procedure.
All of the proposed methods are implemented on Python
using PyTorch \cite{pytorch} framework
for neural fields and ray casting procedure.
The Fairseq \cite{ott2019fairseq} framework is used over the Pytorch,
which provides easier training, evaluation and validation pipelines as well as extends the initial functionality.
During training $80$ pixels are first sampled from each of $1$ or $2$ images 
forming a batch that is used for one training iteration.
These pixels are used to cast rays through the focal point of the corresponding cameras.
These rays then intersect the octree, which is initialized with $N_0^3 = 4^3$ voxels ($4$ voxels for each axis).
These rays are then sampled on the voxel intersection intervals with the step size $0.125$.
The following procedure differs depending on the method
and includes additional light ray casting, intersecting and sampling for the 'brute-force' scheme and BRDF evaluation for the 'explicit schemes'.
Alpha compositing is performed after the color (or radiance) values are achieved.
These values are then being supervised with the ground truth values using the fine-network term from loss function from the \Cref{eq:loss_func} with the beta-distribution regularizer from the \Cref{eq:beta_regularizer}:
\begin{equation}
    \mathcal{L} = \sum_{(p_0,v)\in\mathcal{B}} \norm{ C(p, v) - C^*(p, v) } + \beta(\log(\tau_c(p, v)) + \log(1 - \tau_c(p, v))),
\end{equation}
with $\beta = 10^{-3}$ that is used in the experiments.

Adam optimizer is used as an optimization algorithm for learning parameters $(\Theta, I)$ ($I$ is light intensity value).
The learning rate for the optimizer is initialized with value $10^{-4}$
and linearly decays to value $10^{-6}$ over 100k iterations.
Octree refinement procedure as well as reducing ray casting step size are performed on iterations 5k, 25k and 50k.
Self-pruning is performed every 5k iterations.
Tone-mapping using scaling and gamma-correction (differ depending on the scene)
is performed to visualize HDR images (both inputs and outputs).

All of the experiments are held on NVIDIA GTX 1080Ti GPU with 11GB of memory.
Dataset sample images are originally created in 1024x1024px resolution
and then individually down-sampled to some lower values for each experiment to fit the GPU memory limitations.
The resolutions from 64x64px (for \textit{brute-force scheme}) up to 512x512px 
(for methods with lower memory load, such as Vanilla NSVF) are used.
The \textit{log-transform} is applied at the pre-processing phase
to prepare the HDR data before it is processed by the model:
\begin{equation}
    c = \log (c^{HDR} + 1),
\end{equation}
where $c^{HDR}$ is the color value of the HDR input
and $c$ is the color value that is used for model supervision.
The inverse-log-transform is performed on the model outputs at the post-processing stage.


% \im{64x64 for 'brute force' scheme}
% \im{different schemes - different training times. approx ~12 hours}


% \im{log transform!}


% \begin{enumerate}
%     % \item Loss function: color + beta-regularizer
%     % \item Parameters: optimizer, lr, batch, coefficients, pruning/refinement iterations
%     % \item Software/Hardware setup
%     \item Time estimates
% \end{enumerate}



\section{Metrics}
\label{sec:metrics}

For evaluation of the results the following metrics have been used:
\begin{enumerate}
    \item \textit{PSNR}$\uparrow$ \cite{hore2010image} (Peak Signal to Noise Ratio) is a traditional estimator for image comparison
    that is based on MSE and concentrates on the pixel-by-pixel comparison.
    Higher value is better.
    \item \textit{SSIM}$\uparrow$ \cite{zhou2004image, nilsson2020understanding, hore2010image} (Structural Similarity Index Measure) is a perceptual metric
    that assesses image quality based on perception of the human visual system.
    Higher value is better.
    \item \textit{LPIPS}$\downarrow$ \cite{zhang2018perceptual} (Perceptual Similarity) is the metric
    that utilizes deep features of the pre-trained deep neural networks (Alex-net \cite{krizhevsky2012imagenet} in my experiments)
    to provide an evaluation that agrees well with humans' assessments.
    Lower value is better.
    \item \textit{HDRFlipLoss}$\downarrow$ \cite{theisel2021hdrflip, andersson2020flip} is the metric
    that is aimed to work with HDR images and produce better assessment comparing to other metrics
    that were originally designed to work with LDR images.
    Lower value is better.
\end{enumerate}

Additionally to the above-mentioned metrics, empirical evaluations have to be made
to assess the quality of the reproduced appearance.
This can be done by evaluating the model on a given trajectory for camera and light sources.
This produces continuous changes in appearance and reveals all of the artifacts of the produced renders.
The usage of artificial datasets (\Cref{sec:datasets}) allows generating the ground truth images
even for novel light and view conditions, which are not contained in training or validation datasets.
This cannot be done for real-world images, which makes the whole task of quality assessment more difficult for such datasets.
% Metrics: MSE, SSIM, FLIPLoss/HDRFlipLoss


\section{Experiments}

% The diversity of used models and the corresponding applicable datasets
% does not allow to well structure this section.
% A shortage of some baseline results does also contribute to some difficulties in the results quality assessment.
% Therefore this section if formed in the order of decreasing number of applicable methods (due to its limitations).

% Therefore this section is formed in the order of increasing complexity of used methods and decreasing evaluation capabilities.
% The following methods are being evaluated in either event:
Following list overviews schemes that are compared in this section:
\begin{enumerate}
    \item Vanilla \textit{NSVF} method \cite{liu2021neural} (described in \Cref{subsec:NSVF}).
    The fastest implementation (among other listed methods),
    however, can only be processed with the static light dataset.
    \item \textbf{Ex}plicit \textbf{Col}ocated scheme (\textit{ExCol}) \cite{bi2020neural, liu2021neural}, which is explained in details in \Cref{subsec:NRF}.
    This is a 'lighter' version of the \textit{explicit brute-force scheme}
    due to the reuse of viewing sample points.
    The second fastest implementation as it only contains
    a small overhead of evaluating BRDF instead of directly predicting colors.
    However, similarly to \textit{NSVF} can only be trained on one type of dataset: with the colocated light setting.
    \item \textbf{Ex}plicit \textbf{B}rute-\textbf{F}orce scheme (\textit{ExBF}) (described in \Cref{sec:explicit_scheme}).
    The generalization of \textit{ExCol}.
    The slowest implementation with the highest memory demand,
    highly impractical especially with the aforementioned hardware setup.
    \item \textbf{Ex}plicit \textbf{V}oxel-\textbf{A}pproximated scheme (\textit{ExVA}), which is the \textit{explicit brute-force scheme}
    that uses the in-voxel approximation from \Cref{sec:invoxel}.
    Implementation involves similar complexity as \textit{ExCol}
    as it also includes light rays octree intersection overhead,
    which makes it slower than \textit{ExCol}.
    \item \textbf{Im}plicit \textbf{N}eural \textbf{R}eflectance \textbf{F}ield scheme (\textit{ImNRF}) (described in \Cref{sec:implicit_scheme}).
    This method is based on the Vanilla NSVF approach.
    Light direction vectors and distances to the light source are also provided along with view direction and sample point location.
    This scheme is considered to implicitly represent the appearance of the scene
    and can handle all of the listed dataset settings.
\end{enumerate}


% \CatchFileDef{\catchmethodsdatasets}{tex/objects/figure_dataset_preview.tex}{}
% \catchmethodsdatasets
\input{tex/objects/table_light_settings}

Please refer to \Cref{tab:methods_datasets} for the visual representation of the applicability of different types of datasets to the methods mentioned above methods.






\subsection{Colocated light setting}
\label{subsec:experiments_coloc}

Datasets with the colocated light setting can be evaluated by all of the listed models except \textit{NSVF}.
The fact that a very fast \textit{ExCol} scheme can be evaluated on these datasets
makes them very advantageous for comparing against other methods.
Since the \textit{ExCol} scheme is essentially a special case for the \textit{ExBF} scheme
they perform the same way as long as the sampling for light rays is performed the same way as for view rays.
On the contrary, the performance of the \textit{ExBF} scheme is drastically worse comparing to the \textit{ExCol} scheme.
The \textit{ImNRF} scheme is expected to perform at the same efficiency level as the Vanilla \textit{NSVF} method.
This is due to the fact, that light rays coincide with view rays 
and the scheme does not consider any kind of regularization for the model
to distinguish light rays from view rays.
The light distance is only applicable for light attenuation, 
which is not expected to make a very noticeable effect.
Hence the main focus here is to compare colocated scheme \textit{ExCol} with the in-voxel approximation scheme \textit{ExVA}.



% \CatchFileDef{\catchcolocmetrics}{tex/objects/coloc_metrics.tex}{}
% \catchcolocmetrics
\input{tex/objects/coloc_metrics}

An overview of quantitative results of evaluation methods on colocated datasets is presented in \Cref{tab:colocated_metrics}.
Four aforementioned metrics are calculated on renders over the validation dataset (25\% of the whole dataset, 50 images for these experiments).
\textit{ImNRF} is the fastest scheme since it almost completely shares complexity from Vanilla NSVF.
The \textit{ExCol} scheme is the second fastest
since it does not evaluate any overhead related to sampling light rays
as the view samples are simply reused as samples for light rays.
The overhead of the \textit{ExCol} method comparing with \textit{ImNRF} comes from the additional evaluation of the BRDF model before alpha compositing of view samples.
The \textit{ExVA} scheme is an approximation of the \textit{ExCol} method
that uses the distance of travel of light rays inside the voxels scaled by some constant sigma value.
The implementation implies light rays intersection with the octree,
which is a fairly fast task, although it still substantially drops the performance.
The approximation scheme \textit{ExVA} is from $1.4$ to $2$ times slower
than the fastest \textit{ExCol} depending on the scene.


The \textit{ExCol} scheme is similar to \textit{ExBF} results,
which concurs with the expectations.
However, the \textit{ExBF} method has a massive overhead in sampling light rays
and evaluating the model on these samples,
which makes it naturally impractical.
Training of \textit{ExBF} model even on 64px images takes $1.5$ times more time
than training approximation \textit{ExVA} scheme on 256px images.
In these experiments, the \textit{ExBF} scheme did not manage
to converge completely due to hardware memory limitations.
The \textit{ImNRF} scheme shows the best performance on this dataset.
This is presumably due to the fact that it is not forced
to perform any kind of light interaction generalization.
As can be seen later (\Cref{tab:arb_dynamic_light}) the model
mostly does not change predictions for non-coincident view and light rays.
For these experiments, the \textit{ImNRF} is believed to perform
in a similar way Vanilla \textit{NSVF} would have performed if it used consideration of light interaction.


% % The best values are denoted in bold.
% The \textit{ExCol} scheme does produce mostly better results,
% which should be similar to \textit{ExBF} method.
% However, the \textit{ExBF} method has a massive overhead in sampling light rays
% and evaluating model on these samples,
% which makes it naturally impractical.
% Training of \textit{ExBF} model even on 64px images takes $1.5$ times more time
% than training approximation \textit{ExVA} scheme on 256px images.
% In these experiments the \textit{ExBF} scheme did not manage
% to converge completely due to the hardware memory limitations.
% Interestingly the \textit{ExBF} method performed better according to
% LPIPS metric, even though it did not converge completely.
% This could be the result of same nature of the LPIPS metric
% that uses deep features for the evaluation.
% Some outputs of these experiments are selected from the validation dataset
% and overviewed in \Cref{tab:coloc_allresults}.
% % \textit{ExCol} and \textit{ExVA} methods have been trained using 256x256px images
% % while the brute-force scheme \textit{ExBF} was only trained using 64x64px images.
% % The reason for that is a huge need of the \textit{ExBF} method for memory,
% % which is limited by the hardware.
% The refinement procedure is performed on 5k, 25k and 50k iterations.
% For the \textit{ExBF} method the third refinement procedure requires too much memory,
% so the training only consists of 50k iterations.
% \im{update after ImNRF!}


\input{tex/objects/coloc_results}

Some selected views from these experiments are presented for comparison in \Cref{tab:coloc_allresults}.
As was already said the \textit{ExBF} did not show high-quality results.
The main limitation is that this scheme is very memory-demanding and fairly slow.
Images in 64px resolution have been used for the \textit{ExBF} scheme.
The synthesized views are also in the same resolution and scaled up for clearer comparison.
For Lego and Hotdog datasets \textit{ExBF} failed some parts of scene geometry 
(the corresponding voxels have been removed during the pruning stage).
When comparing \textit{ExCol}, \textit{ExVA} and \textit{ImNRF} schemes,
the results are very similar especially for \textit{ExCol} and \textit{ExVA} methods.
% One can find \textit{ExCol} to reproduce views with more details,
% however the differences are truly slight and could come from the deviation between training iterations.
The \textit{ImNRF} scheme generally showed the best results
(fine details on lego dataset, specularities on hotdog and guitar datasets),
which confirms the tendency from \Cref{tab:colocated_metrics}.
Both \textit{ExCol} and \textit{ExVA} schemes are occasionally lacking specularities (rocket left)
and failing to reproduce very fine details of the Lego dataset.
The very bright spot on the Lego renders (bottom) is also reproduced incorrectly,
however, this is likely due to the models being underfitted.

\subsubsection{ExCol vs. NRF}

The \textit{ExCol} scheme is originally based on the NRF framework \cite{bi2020neural},
which extends the problem of novel view rendering to
'novel view-light rendering' by considering reflectance of the objects in the scene.
However, the \textit{ExCol} method in turn uses the NSVF framework for increasing the efficiency of sampling.
Although \cite{bi2020neural} do not provide the datasets from their experiments,
some rough time estimation comparison can be done.
It takes approximately 48 hours for the NRF model to get trained on 4 GPUs NVIDIA RTX 2080Ti.
This hardware setup is much more powerful than the hardware used in this thesis.
Nevertheless, as it can be seen from \Cref{tab:colocated_metrics}
the \textit{ExCol} can get trained in approximately 12 hours on a single GPU NVIDIA 1080Ti,
which is $4$ times faster than the original NRF approach.
Even considering the differences in the datasets and quantitative differences in achieved results,
the efficiency improvement is fairly distinct.
Unfortunately, \cite{bi2020neural} do not provide any metrics values to compare our results with.




\subsection{Arbitrary light setting}

Datasets with the arbitrary setting cannot be processed with \textit{ExCol} scheme,
since the light source is not co-located with the camera.
The \textit{ExBF} method is an impractically slow generalization for the \textit{ExCol}.
Thus \textit{ExVA} and \textit{ImNRF} are the main methods for achieving renders on this type of datasets.
Although \textit{ExVA} is an approximation,
which theoretically produces worse results than the 'brute-force' or 'colocated' schemes,
experiments with the colocated datasets (\Cref{subsec:experiments_coloc}) showed
that evaluations are very similar between \textit{ExVA} and \textit{ExCol}
by assessing both using metrics (\Cref{tab:colocated_metrics}) and empirically (\Cref{tab:coloc_allresults}).
The implicit scheme is not restricted to represent scene appearance with some specific model.
On the contrary, it is remained to implicitly learn scene appearance
by including light ray directions and distances along with a viewing direction.

\Cref{tab:arb_selective_results} gives an overview of the results of the two mentioned above methods
trained on the Lego dataset with an arbitrary light setting.
Images marked with \textit{HDRFlip} are the difference images produced within HDRFlipLoss \cite{andersson2020flip, theisel2021hdrflip}
between target images and corresponding renders.
It can be seen that both models reproduced the scene with fairly high quality,
although they produce more errors on darker views with some highly specular flares (3rd row, HDRFlip images show strong deviation from target images).
Please note that model predictions, as well as target image on the 3rd row,
are post-processed by scaling and applying gamma-correction ($\gamma = 2.5$)
and in reality, they contain a very dark object with a very bright spot of light reflection.
\textit{ImNRF} shows higher overall performance than \textit{ExVA}.


\subsubsection{Novel light synthesis}

Although \cite{bi2020neural} claim that deep neural network from their framework
can generalize to some arbitrary light position while only being trained on coinciding light-view positions,
the quality of this generalization is still doubtful.
Datasets that were created with the arbitrary light setting
contain denser light-view sample space than those created with the colocated setting.
This results in a higher quality of novel-light synthesized views.
\Cref{tab:arb_dynamic_light} contains five columns,
which contain images evaluated from the same novel (has not been used during training) camera position: $\phi = 0$ and $\theta = 0$.
Each of these images is lit from the light source,
which novel locations are different along different columns.
The light source's location is defined by two angles:
inclination angle $\theta = \const$ and azimuthal angle $\phi$
that takes values from -90\textdegree to 90\textdegree.
In the central column, the light source is co-located with the camera,
which completely coincides with the light setting used for training \textit{ExCol}
(although this view is novel for the method).
Both camera and light positions are the same for renders inside each column.
Please note that for this experiment \textit{ExCol} has been trained on the dataset with colocated light setting
whilst \textit{ExVA} has been trained on the dataset with the arbitrary light setting.
% \textit{Note}: a significant mistake in implementation of \textit{ExCol} was noticed only shortly before the submission deadline.
% For the non-colocated evaluation of the original NRF approach the \textit{light caching} technique is used,
% which reuses volume density from the view samples for the light transmittance calculation.
% To this done, the nearest view sample points are first located at any given shading point
% and then corresponding volume transmittance is linearly interpolated.
% This allows for shadowing effects in original NRF method.
% In these experiments \textit{ExCol} does not interpolate for the light transmittance,
% which means that it does not consider any shadowing effects (which is proved by the results from \Cref{tab:arb_dynamic_light}).

It can be clearly seen that \textit{ImNRF} trained on the colocated light dataset,
did not manage to properly generalize for the non-colocated light-view case.
There also are no shadows on the predictions as \textit{ImNRF} does not consider any shadowing.
In general \textit{ExVA} method outperforms \textit{ExCol}.
The major difference is in the capability of rendering shadows (non-zero angles)
while for the zero-angle view both methods perform with approximately the same quality.
\textit{Note:} that for evaluation under novel light-view conditions the original NRF method used light transmittance caching
(i.e. finding nearest sampled points and interpolating transmittance value between those),
which allowed for realistic shadowing effects.
In \textit{ExCol} \textit{no interpolation is performed}, which reasons the lack of shadows on the presented render.
Another key observation is that the sigma field of \textit{ExVA} did not converge completely on the front part of the base of the model.
\textit{ExCol} renders this part without any difficulties.
This difference is most likely due to the more consistent lighting of this part
captured in the colocated light dataset comparing to those in the arbitrary light dataset.
Another noticeable difference is that \textit{ExVA} better handles fine details on the scene.
This can be explained by the fact that these fine features appear
in the arbitrary light dataset with its own shadows casted in different directions (depending on light source location),
which encourages the density predictor to better distinguish geometrical features from appearance effects.
% Note that due to the colocated nature of training dataset for the \textit{ExCol}
% the model does not happen to meet any shadows at all.
% Another inquiring tendency is how drastically quality of the \textit{ExCol} predictions drops
% when the angle between camera and light rays is increasing.
% This can also be explained by an extremely sparse light-view space of the colocated light dataset.
\textit{ImNRF} (trained on the arbitrary light dataset)
managed to reproduce the scene even better than \textit{ExVA}.
The same tendency as in \Cref{tab:arb_selective_results} can be noticed
that \textit{ImNrf} is able to recover more details comparing to the \textit{ExVA}.



\input{tex/objects/arb_selective_results}

% \input{tex/objects/arb_maps}

\input{tex/objects/arb_dynamic_light}


% \im{RESULTS metrics+renders: check quality from u4108: exva lego 256px, timings can be compared on 64px results: exva u4104 vs exvf u4109}

% \im{Random datasets: compare brute-force with in-voxel approximation}

% The experiments involve training models on different synthetic datasets (\im{reference to prev. subsection}) with different parameters
% and quantitative (\im{ref}) and qualitative (\im{ref}) comparisons of those.




\subsection{Static light setting}

Static light setting is the only type of datasets that can be used
for training the original NSVF method,
due to the limitation of the illumination to be static.
However, this type of dataset appears to be challenging for the novel light-view synthesis task.
Although, the sampling in the light-view space is denser comparing with the colocated setting
where no other than coincided light and view rays exist,
the scene is lit with the light from only one side.
This poorly affects the shadowed side as it always appears in the dataset darkened.
The underlying effect is the worse capabilities of the model to converge on the sigma field,
which plays a significant role in the final appearance estimation.
This problem is not method-dependant and is going to appear on all of the presented results.
However, the incorrect scene geometry will seriously affect the scene under novel lighting
as the shadows will move revealing incorrect parts of the geometry prediction.
% The \textit{ExVA} and \textit{ExBF} models are highly unstable in training on this type of datasets
% and cannot reliably get trained.

This problem is exaggerated by the consideration of using a single light source to illuminate the scene
that comes with formulating the rendering equation (\cref{eq:integral_estimation_nrf}).
In the NeRF and NSVF methods, the global static illumination is considered,
which is not limited by only one light source and can be very complex.

\input{tex/objects/static_valid_results}


% \im{REVIEW below! <- ImNRF results}

Some comparisons of evaluations of these two methods are presented in \Cref{fig:static_valid_results}.
The training was performed on static light datasets.
Each row consists of three sub-rows that contain evaluation outputs for \textit{ExVA}, \textit{NSVF} and \textit{ImNRF} methods.
Columns correspond to different types of evaluation outputs.
\textit{Normal} is the normal map (in the viewing coordinate frame)
that is calculated from the sigma field.
% \textit{Depth} is the corresponding predicted depth map.
\textit{Voxel} map is the visualization of the octree for the presented view rendering.

It can be easily seen that all methods struggle
with the correct prediction of shadowed parts of the scenes
(holes in hotdog dataset, missing part of the base in lego dataset bottom subrow).
Although the geometry fails, the produced renderings
of the \textit{NSVF} and \textit{ImNRF} method match target images with fairly high accuracy.
The reason for that is the static lighting, which is 'baked' into the scene appearance.
On contrary, the \textit{ExVA} method implements the BRDF model,
which relies on geometry and is especially sensitive to correct normal estimation
on spots with high light contribution (i.e. flares from the light source).
This behavior can be seen on the hotdog dataset and guitar dataset.
In general, both \textit{NSVF} and \textit{ImNRF} outperform \textit{ExVA} on this dataset light setting.



\section{Concurrent works results}

Some results of the concurrent works are shown below
in order to make a comparative understanding of our methods' quality.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/concurrent/nerv.png}
    \caption{Novel view renders with a single point light source illumination of (c) NeRV approach \cite{nerv2021}
    in comparison with (a) Ground Truth images and renders of (b) NRV approach \cite{bi2020neural}.
    Images are directly used from NeRV paper \cite{nerv2021}.
    }
    \label{fig:concur_nerv}
\end{figure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/concurrent/nerd.png}
    \caption{Novel view renders with a single point light source illumination of NeRD approach \cite{boss2020nerd}
    in comparison with (a) Ground Truth images.
    Images are directly used from NeRD paper \cite{boss2020nerd}.
    }
    \label{fig:concur_nerd}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/concurrent/nerfactor_3_75percent.png}
    \caption{Novel view renders with a single point light source illumination of NeRFactor approach \cite{nerfactor}
    in comparison with (a) Ground Truth images.
    Images are directly used from NeRFactor paper \cite{nerfactor}.
    }
    \label{fig:concur_nerfactor1}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.82\textwidth]{figures/concurrent/nerfactor_4_75percent.png}
    \caption{Novel view renders with a single point light source illumination of NeRFactor approach \cite{nerfactor}
    in comparison with (a) Ground Truth images.
    Images are directly used from NeRFactor paper \cite{nerfactor}.
    }
    \label{fig:concur_nerfactor2}
\end{figure}
% \section{Discussion}

% \begin{enumerate}
%     % \item HDR inputs: preprocessing (log-transform)
%     \item \im{Background color problem?}
%     -- NSVF used way bigger batch size (32 images instead of 2 in my case) and 2048 rays in each image (80 in my case)
%     % \item In-Voxel approximation: different sigma strategies
%     \item Implicit scheme with tangential coordinate system
% \end{enumerate}

% \im{inference time estimates}

% \im{show albedo/roughness/normals}


% \section{Colocated setting flaws}

% Network barely generalizes for noncolocated light source position when trained only on colocated setting


% \section{Brute Force vs Approximation}






% \section{Blender-Radiance coefficient}

% The light attenuation factor has to be taken into account in the explicit scheme.

% $L_o(x, \Omega_o) = L_e(x, \Omega_o) + \int_{H_i}{f_{BRDF}(\Omega_i, x, \Omega_o) L_i(x, \Omega_i) cos \Theta_i d\omega_i}$

% $L_i = \tau_l L_l$

% $L_l = f_{att}I$

% $f_{att}=\frac{1}{1 + 2d/r + d/r^2}$

% $L_o = \frac{k_d}{\pi} \frac{1}{1 + 2d/r + d/r^2} I cos \Theta_i$

% $k_c = 0, k_l = 0, k_q = 1$

% r = 0.05

% $f_{att} = r^2 / d = 0.0025 / (10-0.05) = 2.51256e-4$

% $0.24316406 = 0.8 / \pi * 2.51256e-4 * I * 1$

% $0.24316406 = 2.01005e-4 * I$

% $I = 500$





% This is some test area for new mathematical helper macros to nicely visualize mathematical formulas.

% \section{Numbers}
% \begin{align}
%     \mathbb{C}
%     \qquad
%     \mathbb{R}
%     \qquad
%     \mathbb{Q}
%     \qquad
%     \mathbb{Z}
%     \qquad
%     \mathbb{N}
% \end{align}

% \section{Numbers with physical units}
% \begin{align}
%     \SI{1.23}{\meter\per\second}
% \end{align}
% \begin{align}
%     \si{\meter\per\second}
% \end{align}
% \begin{align}
%     \SI{1.23\pm0.45}{\meter\per\second}
% \end{align}
% \begin{align}
%     \SI{3e8}{\meter\per\second}
% \end{align}
% \begin{align}
%     \SI{32}{\giga\byte} = \SI{32e9}{\byte}
% \end{align}
% \begin{align}
%     \SI{32}{\gibi\byte} = \SI[exponent-base=2]{32e30}{\byte}
% \end{align}

% \section{Norm, Dot, Abs, Interval}
% \begin{align}
%     \pi = \const
% \end{align}
% \begin{align}
%     1 \in \interval{0}{2}
% \end{align}
% \begin{align}
%     1 \in \order{n}
% \end{align}
% \begin{align}
%     \evalat{ \frac{\partial f}{\partial x} }{ x = 0 }
% \end{align}
% \begin{align}
%     \norm{p} \qquad \norm{\frac{p}{2}}
% \end{align}
% \begin{align}
%     \abs{p} \qquad \abs{\frac{p}{2}}
% \end{align}
% \begin{align}
%     \dotproduct{p}{q} \qquad \dotproduct{\frac{p}{2}}{q}
% \end{align}
% \begin{align}
%     \crossproduct{p}{q} \qquad \crossproduct{\frac{p}{2}}{q}
% \end{align}

% \section{Vector, Matrix}
% \begin{align}
%     \vec{p} \qquad \vecarrow{p}
% \end{align}
% \begin{align}
%     \vec{p}^{\transposed}
% \end{align}
% \begin{align}
%     \gradient{\vec{p}}
% \end{align}
% \begin{align}
%     \divergence{\mat{A}}
% \end{align}
% \begin{align}
%     \laplacian{\mat{A}}
% \end{align}
% \begin{align}
%     \mat{A}
% \end{align}
% \begin{align}
%     \set{K} , K
%     \qquad
%     \set{N} , N
% \end{align}
% \begin{align}
%     \neighborhood{\vec{p}} = \left\{ \vec{q} \mid \norm{\vec{p} - \vec{q}} < \epsilon \right\}
% \end{align}

% \section{Set operations}
% \begin{align}
%     A \intersect B
% \end{align}
% \begin{align}
%     A \union B
% \end{align}
% \begin{align}
%     A \difference B
% \end{align}

% \section{Derivative, Integral, Sum, Probability}
% \begin{align}
%     \int_H x \, dx
% \end{align}
% \begin{align}
%     \sum_H x
% \end{align}
% \begin{align}
%     \probability{x}
% \end{align}
% \begin{align}
%     \probabilitygiven{x}{y}
% \end{align}
% \begin{align}
%     \expectation{x}
% \end{align}
% \begin{align}
%     \deviation{x}
% \end{align}
% \begin{align}
%     \variance{x}
% \end{align}


% \section{Lemma, Theorem, Corollary}
% \begin{lemma}
%     This is a lemma.
% \end{lemma}
% \begin{proof}
%     Proof of lemma.
% \end{proof}

% \begin{theorem}
%     This is a theorem.
% \end{theorem}
% \begin{proof}
%     Proof of theorem.
% \end{proof}

% \begin{corollary}
%     This is a corollary.
% \end{corollary}
% \begin{proof}
%     Proof of corollary.
% \end{proof}