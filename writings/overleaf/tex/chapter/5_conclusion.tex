\chapter{Conclusion}
\label{chap:conclusion}

This thesis focuses on a comprehensive problem of 3D scene reconstruction.
The existing prior works propose different approaches to handle this challenging task,
however, most of them do not give enough quality and performance to be practical.
This work directly addresses the limitations of NeRF-based \cite{mildenhall2020nerf} methods
and elaborates on their extension to also handle Reflectance Fields similarly as proposed by \cite{bi2020neural}.


\section{Contribution}

The main efficiency improvement is connected with using the Neural Sparse Voxel Fields \cite{liu2021neural},
i.e. voxel octree structure together with the encoding
based on the feature vectors that are stored in voxel corners
and corresponding framework consisting of such procedures as \textit{self-pruning} and \textit{refinement}.
This approach is fused with the reformulation of the rendering equation (\Cref{eq:nerf_function})
to consider a single point light source that illuminates the scene as proposed by \cite{bi2020neural}.
This method is called \textit{ExCol} and is the fastest implementation
(among other proposed solutions) that is able to reconstruct the scene under novel light-view conditions.
This scheme nonetheless retains the limitation of the dataset that can be used for training:
it should only consist of co-located light sources
(e.g. sharing the same location with the camera in each image sample).

The 'brute-force scheme' (\textit{ExBF}) is proposed as an \textit{ExCol} generalization
that is able to handle arbitrary light training data
(i.e. when the light source is not restricted to be located at the same position with the camera).
However, this method implies casting many light rays that in turn have to be sampled and evaluated by the model.
This method can be considered applicable, especially on some powerful hardware setups with multiple high-end GPUs,
and comparing with the general formulation of the NRF method \cite{bi2020neural},
the complete impracticality is alleviated and better results can be achieved.
However, it is still highly memory-exhausting, slow when using within accessible hardware setups and thus fairly unstable on training.

Therefore, the approximation to this scheme that leverages the usage of the voxel octree structure is proposed.
The 'brute-force scheme' with the in-voxel approximation is referred to as \textit{ExVA}.
The most computationally expensive part of \textit{ExBF} is the process of
obtaining volume densities for the light rays.
The entire approximation idea is driven by the assumption
that light rays play a secondary role in the contribution to the finally observed value.
Under this assumption, it is claimed that volume transmittance for the light rays
can be estimated by the distance of travel of the light rays inside the octree voxels (\Cref{eq:light_ray_transmittance}).
This allows to almost completely eliminate any overheads comparing to \textit{ExCol}.
Only light rays intersection with the octree is left,
which is efficiently handled using AABB ray intersection algorithm.

The implicit scheme \textit{ImNRF} follows the Vanilla NSVF structure
and learns an implicit representation of the scene during the training phase.
It is generally able to produce better predictions
when comparing with concurrent \textit{ExCol} and \textit{ExVA} methods
as well as showing an outstanding efficiency due to the lack of complications
(i.e. BRDF model evaluation or light rays sampling).
However, \textit{ImNRF} is not regularized to extract appearance from the scene.
This means that \textit{ImNRF} is highly sensitive to the training data
and how dense is the light-view space in it.
Another drawback is the lack of control over the implicit neural representation.


The experiments were held on synthetic datasets listed in \Cref{subsec:scenes}.
The possibility to use the real-world scene in the experiments included the usage of measurements from X-Rite tac7 \cite{merzbach2017highquality}.
However, there were no acquisitions available with calibrated camera poses
and the reconstruction of those produced not accurate enough camera calibrations.
The BARF approach \cite{lin2021barf} could have presumably been better a method for this specific case.
The experiments were hence held on synthetic datasets to compare the performance of corresponding schemes.
Although \textit{ExBF} scheme is the most general and considered to produce the most accurate results,
it appears with questionable applicability and does not impress with the achieved results.
The \textit{ExVA} scheme shows appealing results with approximately the same quality as the accurate and second fastest \textit{ExCol}.
However, \textit{ExVA} implies no limitation for the training dataset,
which is a severe drawback of the \textit{ExCol} method.
In general case comparison of the novel light-view synthesis \textit{ExVA} 
even outperforms \textit{ExCol} by leveraging denser light-view space of the arbitrary light dataset.
The \textit{ImNRF} scheme shows the best efficiency and performance
comparing to all other methods.
However, it is lacking regularization of appearance extraction
(e.g. inability to generalize from the colocated light training data to the arbitrary light validation inputs)
and control over learned representation.



% \im{REVIEW after ImNRF!}


\section{Extension points}

The proposed solutions already achieve considerable improvements in quality and performance.
It can be further developed with a perspective for accelerating training and inference phases
as well as for improving the quality of the predictions.

One way is to adopt the technique proposed by \cite{rebain2020derf},
which applies Voronoi-based decomposition for splitting the scene into sub-scenes
and then applying multiple networks on these parts.
With this done training time is expected to decrease by 2-3 times.

Another extension can be performed by incorporating the auto integration technique proposed by \cite{lindell2021autoint},
which increases the speed of the rendering integral estimation.
In original work, the increase of efficiency is up to a factor of 10.

The usage of different BRDF models can be considered for achieving better results.
For example, in this work, the GGX distribution \cite{walter2007microfacet}
is used for the specular term of the microfacet BRDF model.
However, better results might be obtained by using the symmetric variant of distribution - SGGX \cite{heitz2015sggx}.

% \im{using more light sources?? (e.g. 2 point lights would create even more denser view-light space, while i think \textit{ExVA} is still computationally able to deal with it)}


% \im{mention fail with the flower dataset: (sm) \textit{if you like, you can comment on the real world data in the conclusion: mention that we didn't have any acquisitions available with calibrated camera poses and reconstruction of the camera calibration proved too inaccurate. hence we decided to continue focusing on synthetic data to gain more insights about the different methods instead.}}

\section{Acknowledgements}

% I thank my thesis advisor Sebastian Merzbach for a consistent dedicated involvement throughout the entire process
% and Prof. Dr. Reinhard Klein for steering this work in the right direction
% \textit{\im{and allowing it to be my own work.}}

I thank Prof. Dr. Reinhard Klein for steering this work in the right direction and my thesis advisor Sebastian Merzbach for a consistent dedicated involvement throughout the entire process
I would also like to acknowledge Dr. Michael Weinmann as a second reader of this thesis.
The University of Bonn provided computer pools with GPUs that have been used to manage experiments.
I thank Oleg Kosenko for comments
and CGTrader and Blend Swap users for the 3D models
that have been used to create realistic synthetic datasets:
Heinzelnisse (lego), vernenb (rocket), AshMesh (guitar) and erickfree (hotdog).

% \lipsum[1-15]